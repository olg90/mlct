#Calculation and imaging libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import cv2
import os
import sys
from skimage.io import imread
from skimage.transform import radon, rescale, iradon
import random

#Neural Network Libraries
import keras
from keras.layers import Input, Dense, Conv2D, MaxPooling2D, \
merge, Dropout, UpSampling2D, Cropping2D, Add
from keras.models import Model,Sequential, load_model
from keras.layers import BatchNormalization
from keras.datasets import mnist
from tqdm import tqdm
from keras.layers.advanced_activations import LeakyReLU
from keras.optimizers import Adam

###############################################################################
#
#                             Functions
#
###############################################################################


def def_args(**x):
    """Function to define a dictionary of
    arguments without having to type all the brackets"""
    return x

def get_x_rays(args):
    #Return (img_num) x_ray images reshaped to size (resize_size, resize_size)
    print('Getting {} x-ray images and resizeing them to size {}x{}...'.format(\
        args['img_num'],args['resize_size'],args['resize_size']))
    resize_size = args['resize_size']
    x_rays_dict = {}
    filenames = [x for x in os.listdir()[:args['img_num']] if x[-5:] == '.jpeg']
    for i, v in enumerate(filenames):
        x_rays_dict[i] = cv2.resize(cv2.imread(v, 0), (resize_size, resize_size))
        print('\r{}%'.format(i * 100 / len(filenames)), end='')
        sys.stdout.flush()
    return x_rays_dict

def get_sinograms(imgs, args):
    #Convert images to sinograms
    print('Converting {} x-ray images to sinograms...'.format(args['img_num']))
    sinograms_dict = {}
    for i in range(len(imgs)):
        sinograms_dict[i] = radon(imgs[i], args['angles'])
        print('\r{}%'.format(i * 100 / len(imgs)), end='')
        sys.stdout.flush()
    return sinograms_dict

def get_filters(args, sino_dim):

    block_percentage = args['block_percentage']
    filts = {}
    size_tuple = sino_dim
    for i in range(args['img_num']):
        filt = np.random.choice([True, False], sino_dim,\
            p = [(1 - block_percentage), block_percentage]).reshape(size_tuple)
        #Add 'p' in p = []
        filts[i] = filt
    return filts

def filter_images(images, filters, args):
    filtered_images = {}
    for i in range(args['img_num']):
        filtered_image = np.zeros((images[i].shape[0], images[i].shape[1]))
        filtered_image[filters[i]] = images[i][filters[i]]
        filtered_images[i] = filtered_image
    return filtered_images

def plot_examples(n_examples, images):
    #Plot (n_examples) examples from the dictionary collection of images
    count = 1
    for j in range(n_examples):
        for i,name in enumerate(images.keys()):

            plt.subplot(n_examples, len(images.keys()), count)
            plt.imshow(images[name][j], cmap = 'gray')
            plt.axis('off')
            if (j == 0):
                plt.title(name)
            #Count += 1 is there to change the index of the (m x n) window
            #counting from upper left to right like reading a book
            count += 1
    plt.show()

def apply_FBP(images, args):
    print('Reconstructing images with filtered back projection...')
    recon_dict = {}
    for i in range(len(images)):
        recon_dict[i] = iradon(images[i], args['angles'])
        print('\r{}%'.format(i * 100 / len(images)), end='')
        sys.stdout.flush()
    return recon_dict

###############################################################################
#
#                            Set up the imaging data
#
###############################################################################

args = def_args(
    img_num = 100,
    img_size = 64,
    block_percentage = 0.05,
    resize_size = 256,
    angles = np.linspace(0, 180, 180),
	#Home address
    address = 'C:/Users/Oliver/Desktop/X-rays',
	
	#Work address
	#address = 'C:/Users/Oliver.ANGELA-PC/Desktop/X-ray codes'
    n_inits = 100,
    train_percentage = 0.8,
)

os.chdir(args['address'])

x_rays = get_x_rays(args)
sinograms = get_sinograms(x_rays, args)
sino_dim = sinograms[0].shape
filters = get_filters(args, sino_dim)
filtered_images = filter_images(sinograms, filters, args)
recon_FBP = apply_FBP(sinograms, args)
errors = [abs(x_rays[i] - recon_FBP[i]) for i in range(len(x_rays))]

sinograms_shape = sinograms[0].shape

images = def_args(x_rays = x_rays, sinograms = sinograms, \
                   filters = filters, filtered_images = filtered_images, \
                  recon_FPB = recon_FBP, errors = errors)

plot_examples(3, images)

###############################################################################
#
#                     Set up the training and testing data
#
###############################################################################

train_num = int(args['train_percentage'] * args['img_num'])
random_selection = random.sample(range(args['img_num']), args['img_num'])
train_selection = random_selection[:train_num]
test_selection = random_selection[train_num:]

X_train = np.array([list(filtered_images.values())[i] for i in train_selection])
y_train = np.ones(train_num)

X_test = np.array([list(filtered_images.values())[i] for i in test_selection])
y_test = np.ones(len(test_selection))

data = def_args(X_train = X_train, y_train = y_train,
                X_test = X_test, y_test = y_test)

def GAN(data, args):   

###############################################################################
#
#                              Final GAN
#
###############################################################################

def adam_optimizer():
    return Adam(lr=0.0002, beta_1=0.5)

def create_generator(sinograms_shape):


    ss0 = sinograms_shape[0]
    ss1 = sinograms_shape[1]

    inputs_3 = Input(shape=(ss0,ss1,1))

    x3 = Conv2D(256, (3,3), padding = 'same',activation='relu')(inputs_3)
    x3 = BatchNormalization()(x3)
    x3 = Conv2D(128, (3,3), padding = 'same',activation='relu')(x3)
    x3 = BatchNormalization()(x3)
    x3 = Conv2D(64, (3,3), padding = 'same',activation='relu')(x3)
    x3 = BatchNormalization()(x3)
    x3 = Conv2D(32, (3,3), padding = 'same', activation = 'relu')(x3)
    x3 = BatchNormalization()(x3)
    x3 = Conv2D(16, (3,3), padding = 'same', activation = 'relu')(x3)
    x3 = BatchNormalization()(x3)
    x3 = Conv2D(8, (3,3), padding = 'same', activation = 'relu')(x3)
    x3 = BatchNormalization()(x3)
    x3 = Conv2D(1, (3,3), padding = 'same', activation = 'relu')(x3)

    model_3 = Model(inputs=inputs_3, outputs=x3)






    generator=Sequential()
    #Inputs of size 100
    generator.add(Dense(units=256,input_dim=100))
    generator.add(LeakyReLU(0.2))
    
    generator.add(Dense(units=512))
    generator.add(LeakyReLU(0.2))
    
    generator.add(Dense(units=1024))
    generator.add(LeakyReLU(0.2))
    
    generator.add(Dense(units=784, activation='tanh'))
    
    generator.compile(loss='binary_crossentropy', optimizer=adam_optimizer())
    return generator
g=create_generator()
g.summary()




###############################################################################
#
#                              TESTING GAN below here
#
###############################################################################

def load_data():
    (x_train, y_train), (x_test, y_test) = mnist.load_data()
    x_train = (x_train.astype(np.float32) - 127.5)/127.5
    
    # convert shape of x_train from (60000, 28, 28) to (60000, 784) 
    # 784 columns per row
    x_train = x_train.reshape(60000, 784)
    return (x_train, y_train, x_test, y_test)

(X_train, y_train,X_test, y_test)=load_data()
print(X_train.shape)

def adam_optimizer():
    return Adam(lr=0.0002, beta_1=0.5)

def create_generator():
    generator=Sequential()
    #Inputs of size 100
    generator.add(Dense(units=256,input_dim=100))
    generator.add(LeakyReLU(0.2))
    
    generator.add(Dense(units=512))
    generator.add(LeakyReLU(0.2))
    
    generator.add(Dense(units=1024))
    generator.add(LeakyReLU(0.2))
    
    generator.add(Dense(units=784, activation='tanh'))
    
    generator.compile(loss='binary_crossentropy', optimizer=adam_optimizer())
    return generator
g=create_generator()
g.summary()


def create_discriminator():
    discriminator=Sequential()
    #Image size is of dimensions 784
    discriminator.add(Dense(units=1024,input_dim=784))
    discriminator.add(LeakyReLU(0.2))
    discriminator.add(Dropout(0.3))
       
    
    discriminator.add(Dense(units=512))
    discriminator.add(LeakyReLU(0.2))
    discriminator.add(Dropout(0.3))
       
    discriminator.add(Dense(units=256))
    discriminator.add(LeakyReLU(0.2))
    
    discriminator.add(Dense(units=1, activation='sigmoid'))
    
    discriminator.compile(loss='binary_crossentropy', optimizer=adam_optimizer())
    return discriminator
d =create_discriminator()
d.summary()

def create_gan(discriminator, generator):
    discriminator.trainable=False
    gan_input = Input(shape=(100,))
    x = generator(gan_input)
    gan_output= discriminator(x)
    gan= Model(inputs=gan_input, outputs=gan_output)
    gan.compile(loss='binary_crossentropy', optimizer='adam')
    return gan
gan = create_gan(d,g)
gan.summary()


import os
os.chdir('C://Users/Oliver/Desktop/')
os.chdir(os.getcwd() + '/Test')

def plot_generated_images(epoch, generator, examples=100, dim=(10,10), figsize=(10,10)):
    noise= np.random.normal(loc=0, scale=1, size=[examples, 100])
    generated_images = generator.predict(noise)
    generated_images = generated_images.reshape(100,28,28)
    plt.figure(figsize=figsize)
    for i in range(generated_images.shape[0]):
        plt.subplot(dim[0], dim[1], i+1)
        plt.imshow(generated_images[i], interpolation='nearest')
        plt.axis('off')
    plt.tight_layout()
    plt.savefig('gan_generated_image %d.png' %epoch)


def training(epochs=1, batch_size=128):
    
    #Loading the data
    (X_train, y_train, X_test, y_test) = load_data()
    batch_count = X_train.shape[0] / batch_size
    
    # Creating GAN
    generator= create_generator()
    discriminator= create_discriminator()
    gan = create_gan(discriminator, generator)
    
    for e in range(1,epochs+1 ):
        print("Epoch %d" %e)
        #tqdm shows a progress meter
        for _ in tqdm(range(batch_size)):
        #generate  random noise as an input  to  initialize the  generator
            noise= np.random.normal(0,1, [batch_size, 100])
            
            # Generate fake MNIST images from noised input
            generated_images = generator.predict(noise)
            
            # Get a random set of  real images
            image_batch = X_train[np.random.randint(low=0,high=X_train.shape[0],size=batch_size)]
            
            #Construct different batches of  real and fake data 
            #each mini-batch needs to contain only all real images or all generated images
            
            #Stack (128, 784) image_batch on top of (128, 784) generated images
            X= np.concatenate([image_batch, generated_images])
            
            # Labels for generated and real data
            y_dis=np.zeros(2*batch_size)
            y_dis[:batch_size]=0.9
            
            #Pre train discriminator on  fake and real data  before starting the gan. 
            discriminator.trainable=True
            discriminator.train_on_batch(X, y_dis)
            
            #Tricking the noised input of the Generator as real data
            noise= np.random.normal(0,1, [batch_size, 100])
            y_gen = np.ones(batch_size)
            
            # During the training of gan, 
            # the weights of discriminator should be fixed. 
            #We can enforce that by setting the trainable flag
            discriminator.trainable=False
            
            #training  the GAN by alternating the training of the Discriminator 
            #and training the chained GAN model with Discriminatorâ€™s weights freezed.
            gan.train_on_batch(noise, y_gen)
            
        if e == 1 or e % 20 == 0:
           
            plot_generated_images(e, generator)
training(400,128)
